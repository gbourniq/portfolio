                <meta charset="utf-8" emacsmode="-*- markdown -*-">
                            **AWS EC2 Deployment Guide**


!!!
    If you're viewing this html file from Github, the page may not render properly.
    Please open the [online version](https://portfoliogb.s3.eu-west-2.amazonaws.com/documentation/ec2_deployment_guide.html).


Overview
===============================================================================

This page provides instructions to deploy the containerized [portfolio app](https://github.com/gbourniq/portfolio/) using [AWS Free Tier services](https://aws.amazon.com/free/).

The deployment is based on the *prod.docker-compose.yaml* file, which spins up the following containers:
- Nginx as a reverse proxy
- Django webserver
- Celery worker
- Redis for caching and as a message broker
- Postgres database


Configure AWS Services
===============================================================================

Create AWS account
-------------------------------------------------------------------------------
Create an AWS account using this link: https://aws.amazon.com/free/.

For this deployment, we are interested in the following Free Tier services:
- 750 hours / month of AWS EC2 (t2.micro)
- 5GB of AWS S3 storage 
- Elastic IP


Create an IAM User
-------------------------------------------------------------------------------
Create an [IAM User](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html) from the AWS Management Console, in order to enable a programmatic access to our AWS services. 

After the user creation, navigate to *Security credentials*, and click *Create Access Key*.

!!! WARNING
    Please take a note of the **Access Key ID** and **Secret Access Key**.

![IAM User Dashboard](images/aws_iam_user_page.png)


Create S3 Bucket storage
-------------------------------------------------------------------------------
On the [AWS S3 Console](https://s3.console.aws.amazon.com/), click *Create Bucket*.

On the general configuration page, enter a meaningful *Bucket name* and your AWS region. 

!!! WARNING
    Please take a note of the **Bucket Name** and **AWS Region**.

![Create AWS S3](images/aws_create_s3_bucket.png)

!!!
    **This S3 Bucket will store the following blobs**

    - Django media and static files to serve
    - Postgres data dumps
    - Docker deployment tarball
    - SSL Cert and Private key for Nginx


Launch ec2 instance (t2.micro)
-------------------------------------------------------------------------------
On the [EC2 Dashboard](https://eu-west-3.console.aws.amazon.com/ec2), select your *AWS Region* from the top-right drop-down menu, then click *Launch Instance*.

![EC2 Dashboard - Select Region and Launch Instance](images/aws_ec2_dashboard_select_region.png)

The ec2 instance must be configured with the following parameters:

AMI
:       Ubuntu Server 18.04 LTS (HVM), SSD Volume Type.
Type
:       t2.micro (Free tier eligible)
General
:       Leave defaults
Storage
:       8GB (default)
Tag
:       You may add a tag such as *"Name": "Portfolio App | Prod | 08-05-2020"*
SG
:       Create a Security Group to allow traffic via SSH/HTTP/HTTPS, and a custom 587 port for Gmail SMTP

On the last tab, click the *Launch* button, this will open a new window to create the **key pair**, which is required to access ec2 instances.

Enter a meaningful name for the key, eg. *aws_private_key_london*, and download the private key file (*.pem).

Run the following commands to make the key not publicly viewable (required for SSH) and move it to a hidden *.ssh/* folder:
~~~~~~bash
chmod 400 aws_private_key.pem
mv aws_private_key.pem ~/.ssh/
~~~~~~
After the private key pair is downloaded, the *launch* button will be enable. 

Create an Elastic IP
-------------------------------------------------------------------------------
An Elastic IP address is a static, public IPv4 address, that can be assigned to an EC2 instance. This prevent the ec2 IPv4 address to change when it is restarted.

From the [EC2 Dashboard](https://eu-west-3.console.aws.amazon.com/ec2), navigate to *Network & Security > Elastic IPs* on the left panel, and click *Allocate Elastic IP Address*

Navigate back to the EC2 instance panel, and verify that the *IPv4 Public IP* value points to the static *Elastic IP address*.

![EC2 Instance Panel showing IPv4 Public IP](images/aws_ec2_instance_details.png)


Enable SSH password for Ansible (optional)
-------------------------------------------------------------------------------
An SSH password must be created for Ansible to access the EC2 instance.

First, ssh into the instance with:
~~~~~~bash
ssh -i ~/.ssh/<private-key-file.pem> ubuntu@<IPv4-public-ip>
~~~~~~

To enable SSH password authentication, you must change the line *PasswordAuthentication no* to *PasswordAuthentication yes* in */etc/ssh/sshd_config*.
After making that change, restart the SSH service and set your SSH password.
~~~~~~bash
sudo vim /etc/ssh/sshd_config
sudo service ssh restart
sudo passwd ubuntu
~~~~~~

!!! WARNING
    Please take a note of the **SSH password** (required by Ansible playbooks).

Verify that the ssh password has been set correctly by logging back in without the private key:
~~~~~~bash
exit
ssh ubuntu@<IPv4-public-ip> 
~~~~~~

Install required packages on ec2
-------------------------------------------------------------------------------
The following packages are necessary for running the later deployment steps. 

~~~~~~bash
### Install Docker 
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo apt-key fingerprint 0EBFCD88
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable"
sudo apt-get update
sudo apt-get install docker-ce=18.03.*
sudo groupadd docker
sudo usermod -aG docker ubuntu

### Install docker-compose
sudo curl -L "https://github.com/docker/compose/releases/download/1.25.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose --version

### Install required ubuntu packages 
sudo apt-get install make curl awscli
~~~~~~


Set up a domain name with AWS Route 53
-------------------------------------------------------------------------------
1. Register a **domain name** from any online provider

At the time of writing, the cost of a domain name + SSL certification is around ~£15/year with [Namecheap](https://www.namecheap.com). 

!!!
    Upon successful registration of a domain name, you should receive a **CA-signed certificate** (.cert) and the **SSL private key** (.key). 


These two files are expected inside the *docker-deployment/nginx/certs/* directory for nginx to be configured correctly (*prod build*).

2. Create a **hosted zone** in [AWS Route 53](https://console.aws.amazon.com/route53/home?region=eu-west-2#) to associate a domain name to an EC2 instance:

    1. Click *Create Hosted Zone*, two record set (NS and SOA) will be created automatically.
    2. Click on the *NS* type, copy the 4 Amazon NS urls (eg. ns-396.awsdns-49.com), and paste them into the NameCheap DNS settings.
    3. Create two new *A* or *A – IPv4 address* records (root domain and www.domain) and point it to the Elastic IP.

Traffic to/from the application will then be handled as follows:

****************************************************************************************
* .---------.   .-----------.   .-----------.   .---------.   .------------.   .-----. *
* | Request +<->+ NameCheap +<->+ Amazon NS +<->+ Route53 +<->+ Elastic IP +<->+ EC2 | *
* '---------'   '-----------'   '-----------'   '---------'   '------------'   '-----' *
****************************************************************************************

![Configurations for Hosted Zone in AWS 53](images/aws_route_53_hosted_zone.png)

Prepare Application
===============================================================================

Configure environment variables
-------------------------------------------------------------------------------
Set the following sensitive variables locally in your **~/.bash_profile** (and in Travis CI settings):
- **ANSIBLE_SSH_PASSWORD**
- **AWS_ACCESS_KEY_ID**
- **AWS_SECRET_ACCESS_KEY**

The environment variables below can be set in the repository **.env** file:
- **AWS_DEFAULT_REGION**
- **AWS_STORAGE_BUCKET_NAME**  
- **AWS_ENABLED=True**
- **ENABLE_S3_FOR_DJANGO_FILES=True**


Run pytest
-------------------------------------------------------------------------------
Verify that all unit-tests are passing
~~~~~~bash
conda activate portfolio
source .dev.env
pytest -v
~~~~~~

Run CI/CD pipeline
-------------------------------------------------------------------------------
Run the CI/CD pipeline locally to ensure docker services are healthy (*dev build*)
~~~~~~bash
source .dev.env
make run-ci-pipeline
~~~~~~

Build and upload the docker deployment tarball to S3
-------------------------------------------------------------------------------
The docker deployment tarball is a compressed folder (.tar.gz) containing all necessary files for a docker-compose deployment (*prod build*).

To build the package and upload it to S3, run the following command: 
~~~~~~bash
make docker-deploy-tarball-custom
~~~~~~

!!!
    Verify that the docker deployment tarball

    - Has been created successfully in */bin*
    - Has been uploaded to *https://s3.console.aws.amazon.com/s3/*


Configure nginx.conf with domain name
-------------------------------------------------------------------------------
Configure your nginx service by replacing existing values with your own domain name in [](./deployment/docker-deployment/nginx/conf.d/nginx.conf).


Upload SSL private key and certificate to S3
-------------------------------------------------------------------------------
Upload the **CA-signed certificate** (.cert) and SSL **private key** (.key) generated in step 2.8, to an **ssl_certs/** folder in S3.

If you wish to use the Ansible playbook, please also update the *SSL_CERT_S3_OBJECT_PATH* and *SSL_KEY_S3_OBJECT_PATH* environment variables in **.dev.env**. Eg:
~~~~~~bash
export SSL_CERT_S3_OBJECT_PATH=ssl_certs/cert_chain.crt
export SSL_KEY_S3_OBJECT_PATH=ssl_certs/www_gbournique_com.key
~~~~~~




Deployment Steps
===============================================================================

SSH into ec2 instance
-------------------------------------------------------------------------------
SSH into the instance and make sure the current working directory is */home/ubuntu/*
~~~~~~bash
ssh -i ~/.ssh/<private-key-file.pem> ubuntu@<IPv4-public-ip>
pwd
~~~~~~

Download docker deployment tarball from S3
-------------------------------------------------------------------------------
Download the tarball file created from step 3.4:
~~~~~~bash
wget https://portfoliogb.s3.eu-west-2.amazonaws.com/docker_deploy_tarballs/docker_deploy_gbournique.tar.gz
~~~~~~

Untar it, rename the folder to *portfolio/*, and cd into it
~~~~~~bash
tar -xvf docker_deploy_gbournique.tar.gz
mv docker_deploy_gbournique portfolio
cd portfolio/
~~~~~~

!!!
    Please note the above urls are given as an example

Download SSL cert and private key from S3
-------------------------------------------------------------------------------
Download the SSL files uploaded in step 3.6, and move them to the **certs/** directory so they can be mounted into the nginx container.
~~~~~~bash
wget https://portfoliogb.s3.eu-west-2.amazonaws.com/ssl_certs/cert_chain.crt
wget https://portfoliogb.s3.eu-west-2.amazonaws.com/ssl_certs/www_gbournique_com.key
mv cert_chain.crt www_gbournique_com.key docker-deployment/nginx/certs/
~~~~~~

!!!
    Please note the above urls are given as an example

Review environment variables and source .env 
-------------------------------------------------------------------------------
Verify that all environment variables are correctly configured in **.env**.

Edit the .env file with vim
~~~~~~bash
cd portfolio/
vim .env # if requires changes
~~~~~~

And populate the following environment variables:
~~~~~~bash
export EMAIL_HOST_USER=
export EMAIL_HOST_PASSWORD=
export AWS_ACCESS_KEY_ID=
export AWS_SECRET_ACCESS_KEY=
~~~~~~

!!!
    Please review other variables such as IMAGE_REPOSITORY to pull the app image

Save and quit the vim editor and source the file
~~~~~~bash
source .env
~~~~~~


Start docker services for prod build
-------------------------------------------------------------------------------
Start the services with the following make command
~~~~~~bash
make up
~~~~~~

Ensure docker services are healthy
~~~~~~bash
make check-services-health
~~~~~~

Optionally check that the postgres back up scripts work correctly
~~~~~~bash
make postgres-dump-to-s3
make postgres-restore-from-s3
~~~~~~
!!!
    Postgres backup scripts require to configure the AWS CLI on the EC2 instance, by running the *aws configure* command. 


Set Crontab to run the app on instance start up
-------------------------------------------------------------------------------
When the instance is restarted, it will not start the docker services by default.

To enable this functionality, edit your crontab with *crontab -e* and add the following entry:

~~~~~~bash
@reboot /home/ubuntu/portfolio/deployment/start_docker_services.sh
~~~~~~


<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
